{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Infection Risk from CPT, Microbiology, and Lab Events\n",
    "This notebook builds a machine learning and deep learning pipeline to predict infection risk in ICU patients using MIMIC-III data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/p_ltq8v135ggx_jn7q66xr680000gn/T/ipykernel_40009/4065733134.py:2: DtypeWarning: Columns (4,5,7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cptevents = pd.read_csv('CPTEVENTS.csv')\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Data (replace paths as needed)\n",
    "cptevents = pd.read_csv('CPTEVENTS.csv')\n",
    "microbio = pd.read_csv('MICROBIOLOGYEVENTS.csv')\n",
    "labevents = pd.read_csv('LABEVENTS.csv')\n",
    "admissions = pd.read_csv('ADMISSIONS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Target Variable (Infection Risk)\n",
    "infected_hadm_ids = microbio[microbio['ORG_NAME'].notnull()]['HADM_ID'].unique()\n",
    "admissions['infection_label'] = admissions['HADM_ID'].isin(infected_hadm_ids).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering\n",
    "# CPT Code Features\n",
    "cpt_features = cptevents.groupby(['HADM_ID', 'CPT_CD']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Event Features\n",
    "important_items = [50811, 50912, 50931]  # WBC, Glucose, Lactate\n",
    "lab_filtered = labevents[labevents['ITEMID'].isin(important_items)]\n",
    "lab_agg = lab_filtered.groupby(['HADM_ID', 'ITEMID'])['VALUENUM'].agg(['mean', 'min', 'max']).unstack()\n",
    "lab_agg.columns = ['{}_{}'.format(item, stat) for item, stat in lab_agg.columns]\n",
    "lab_agg = lab_agg.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Combine Features and Labels\n",
    "features = cpt_features.join(lab_agg, how='outer').fillna(0)\n",
    "features = features.join(admissions.set_index('HADM_ID')['infection_label'], how='inner')\n",
    "features.columns = features.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: ML Modeling\n",
    "X = features.drop(columns=['infection_label'])\n",
    "y = features['infection_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.91      0.81      6270\n",
      "           1       0.78      0.47      0.59      4065\n",
      "\n",
      "    accuracy                           0.74     10335\n",
      "   macro avg       0.75      0.69      0.70     10335\n",
      "weighted avg       0.75      0.74      0.72     10335\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devanshkochar/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(\"Logistic Regression Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.81      6270\n",
      "           1       0.73      0.56      0.64      4065\n",
      "\n",
      "    accuracy                           0.75     10335\n",
      "   macro avg       0.74      0.71      0.72     10335\n",
      "weighted avg       0.75      0.75      0.74     10335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare Lab Sequences for LSTM\n",
    "def create_lab_sequence(hadm_id, item_ids, max_len=24):\n",
    "    patient_data = lab_filtered[lab_filtered['HADM_ID'] == hadm_id]\n",
    "    seq = []\n",
    "    for item in item_ids:\n",
    "        values = patient_data[patient_data['ITEMID'] == item].sort_values('CHARTTIME')['VALUENUM'].values\n",
    "        padded = np.pad(values[:max_len], (0, max(0, max_len - len(values))), 'constant', constant_values=0)\n",
    "        seq.append(padded)\n",
    "    return np.stack(seq, axis=1)  # shape: (max_len, num_features)\n",
    "\n",
    "hadm_ids = admissions['HADM_ID'].unique()\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "for hid in hadm_ids:\n",
    "    if hid in admissions['HADM_ID'].values:\n",
    "        try:\n",
    "            seq = create_lab_sequence(hid, important_items)\n",
    "            X_seq.append(seq)\n",
    "            y_seq.append(int(hid in infected_hadm_ids))\n",
    "        except:\n",
    "            continue\n",
    "X_seq = np.stack(X_seq)\n",
    "y_seq = np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = nan\n",
      "Epoch 2: loss = nan\n",
      "Epoch 3: loss = nan\n",
      "Epoch 4: loss = nan\n",
      "Epoch 5: loss = nan\n",
      "Epoch 6: loss = nan\n",
      "Epoch 7: loss = nan\n",
      "Epoch 8: loss = nan\n",
      "Epoch 9: loss = nan\n",
      "Epoch 10: loss = nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Normalize input features\n",
    "X_seq = (X_seq - X_seq.mean(axis=0)) / (X_seq.std(axis=0) + 1e-8)\n",
    "X_seq = np.nan_to_num(X_seq, nan=0.0)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_seq, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Dataset and dataloaders\n",
    "train_ds = TensorDataset(X_tensor[:int(0.8 * len(X_tensor))], y_tensor[:int(0.8 * len(X_tensor))])\n",
    "test_ds = TensorDataset(X_tensor[int(0.8 * len(X_tensor)):], y_tensor[int(0.8 * len(X_tensor)):])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "# LSTM model definition\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])  # raw logits\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMClassifier(input_dim=X_seq.shape[2], hidden_dim=32, num_layers=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: loss = {epoch_loss / len(train_dl):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
